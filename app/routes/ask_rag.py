from fastapi import APIRouter, Query, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from app.database import get_db
from app.models import Embedding, QuestionAnswer
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime
import os
import numpy as np
import json
from sklearn.metrics.pairwise import cosine_similarity
import tiktoken
import html
from app.auth import get_current_user_id, verify_student  # âœ… ì¶”ê°€

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
router = APIRouter()

# âœ… tokenizer ì„¤ì • (gpt-3.5-turbo ê¸°ì¤€)
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# âœ… ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
def generate_query_embedding(query: str) -> list:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )
    return response.data[0].embedding

# âœ… ìœ ì‚¬í•œ chunk ì¶”ì¶œ
def get_top_chunks(query_vec, embedding_objs, top_n=5):
    db_vectors = np.array([json.loads(e.embedding) for e in embedding_objs])
    similarities = cosine_similarity([query_vec], db_vectors)[0]
    top_indices = similarities.argsort()[-top_n:][::-1]
    return [embedding_objs[i] for i in top_indices]

# âœ… context êµ¬ì„±
def build_context(chunks, max_total_tokens=3000):
    context = ""
    total_tokens = 0
    for chunk in chunks:
        chunk_text = chunk.content.strip()
        chunk_tokens = count_tokens(chunk_text)
        if total_tokens + chunk_tokens <= max_total_tokens:
            context += chunk_text + "\n"
            total_tokens += chunk_tokens
        else:
            break
    return context

# âœ… ì§ˆë¬¸ ì²˜ë¦¬ ë¼ìš°í„°
@router.get("/ask_rag")
async def ask_rag(
    q: str = Query(..., description="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"),
    db: AsyncSession = Depends(get_db),
    user_id: int = Depends(get_current_user_id),
    _: str = Depends(verify_student)  # âœ… í•™ìƒë§Œ ì ‘ê·¼ ê°€ëŠ¥
):
    """RAG ë°©ì‹ìœ¼ë¡œ ê°•ì˜ìë£Œ ê¸°ë°˜ GPT ë‹µë³€ ì œê³µ"""

    # âœ… ë™ì¼ ì‚¬ìš©ìê°€ ì´ë¯¸ ì§ˆë¬¸í•œ ì  ìˆë‹¤ë©´ ìºì‹± ì‘ë‹µ
    existing = await db.execute(
        select(QuestionAnswer).where(
            QuestionAnswer.question == q,
            QuestionAnswer.user_id == user_id
        )
    )
    answer_obj = existing.scalar_one_or_none()
    if answer_obj:
        return {"answer": answer_obj.answer}

    try:
        query_embedding = generate_query_embedding(q)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")

    result = await db.execute(select(Embedding))
    embedding_chunks = result.scalars().all()
    if not embedding_chunks:
        raise HTTPException(status_code=404, detail="ì €ì¥ëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")

    top_chunks = get_top_chunks(query_embedding, embedding_chunks, top_n=5)
    context = build_context(top_chunks, max_total_tokens=3000)
    

    prompt = f"""
ì•„ë˜ ê°•ì˜ìë£Œ ë°œì·Œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ë‹µë³€í•˜ì„¸ìš”. ì¤„ë°”ê¿ˆì€ <br>ë¡œ í‘œì‹œí•˜ì„¸ìš”.

--- ìë£Œ ì‹œì‘ ---
{context}
--- ìë£Œ ë ---

ì§ˆë¬¸: {"ê°ì²´ì§€í–¥ì´ ë­ì•¼"}
ë‹µë³€:
"""
    print(f"ğŸ“ Context ê¸¸ì´ (ë¬¸ì ìˆ˜): {len(prompt)}")

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.7,
        )
        raw_answer = response.choices[0].message.content.strip()
        escaped_answer = html.escape(raw_answer)
        formatted_answer = escaped_answer.replace("\n", "<br>")

        new_qa = QuestionAnswer(
            question=q,
            answer=formatted_answer,
            created_at=datetime.utcnow(),
            user_id=user_id
        )
        db.add(new_qa)
        await db.commit()

        return {"answer": formatted_answer}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPT ì‘ë‹µ ì‹¤íŒ¨: {str(e)}")

