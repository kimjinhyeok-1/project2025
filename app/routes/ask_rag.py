from fastapi import APIRouter, Query, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from app.database import get_db
from app.models import QuestionAnswer, Embedding
from openai import OpenAI
from datetime import datetime
from dotenv import load_dotenv
import os
import numpy as np
import faiss
import tiktoken
import pickle
from app.auth import get_current_user_id, verify_student

# ======================= í™˜ê²½ ì„¤ì • =======================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
router = APIRouter()

# ======================= ì „ì—­ ìƒíƒœ =======================
cached_embeddings = []
embedding_id_map = []
faiss_index = {"index": None}
query_cache = {}

# ======================= FAISS ì €ìž¥/ë¶ˆëŸ¬ì˜¤ê¸° =======================
def save_faiss_index(path="faiss_index.bin"):
    if faiss_index["index"] is not None:
        faiss.write_index(faiss_index["index"], path)

def load_faiss_index(path="faiss_index.bin"):
    if os.path.exists(path):
        faiss_index["index"] = faiss.read_index(path)

# ======================= GPT ìž„ë² ë”© + í† í¬ë‚˜ì´ì € =======================
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

def generate_query_embedding(query: str) -> list:
    if query in query_cache:
        return query_cache[query]
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        embedding = response.data[0].embedding
        query_cache[query] = embedding
        return embedding
    except Exception as e:
        raise RuntimeError(f"ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")

# ======================= ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ =======================
def summarize_chunk_with_question(chunk_text: str, question: str) -> str:
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "ì•„ëž˜ ê°•ì˜ìžë£Œ ë‚´ìš© ì¤‘ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ 1~2ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"ì§ˆë¬¸: {question}\në‚´ìš©: {chunk_text}"
                }
            ],
            max_tokens=150,
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return "[ìš”ì•½ ì‹¤íŒ¨: ì›ë¬¸ ì‚¬ìš©] " + chunk_text[:200]

# ======================= FAISS ê¸°ë°˜ ê²€ìƒ‰ =======================
def get_top_chunks_faiss(query_vec, top_n=5):
    if faiss_index["index"] is None:
        raise RuntimeError("FAISS ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    query_np = np.array([query_vec]).astype("float32")
    distances, indices = faiss_index["index"].search(query_np, top_n)

    top_chunks = []
    for idx, dist in zip(indices[0], distances[0]):
        embedding_id = embedding_id_map[idx]
        matched = next((e for e in cached_embeddings if e.id == embedding_id), None)
        if matched:
            matched.similarity = 1 - dist
            top_chunks.append(matched)
    return top_chunks

# ======================= Context ìƒì„± =======================
def build_context(chunks, question: str, max_total_tokens=3000):
    context = ""
    total_tokens = 0
    used_ids = set()

    for chunk in chunks:
        if chunk.id in used_ids:
            continue

        chunk_text = chunk.content.strip()
        chunk_tokens = count_tokens(chunk_text)

        # ê¸´ chunkëŠ” ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½
        if chunk_tokens > 300:
            chunk_text = summarize_chunk_with_question(chunk_text, question)
            chunk_tokens = count_tokens(chunk_text)

        if total_tokens + chunk_tokens <= max_total_tokens:
            context += chunk_text + "\n"
            total_tokens += chunk_tokens
            used_ids.add(chunk.id)
        else:
            break
    return context

# ======================= ì§ˆë¬¸ API =======================
@router.get("/ask_rag")
async def ask_rag(
    q: str = Query(..., description="ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”"),
    db: AsyncSession = Depends(get_db),
    user_id: int = Depends(get_current_user_id),
    _: str = Depends(verify_student)
):
    # 1. ìºì‹œëœ ë‹µë³€ í™•ì¸
    existing = await db.execute(
        select(QuestionAnswer).where(
            QuestionAnswer.question == q,
            QuestionAnswer.user_id == user_id
        )
    )
    cached_answer = existing.scalar_one_or_none()
    if cached_answer:
        return {"answer": cached_answer.answer}

    # 2. ìž„ë² ë”© ìƒì„±
    try:
        query_embedding = generate_query_embedding(q)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

    # 3. FAISS ìœ ì‚¬ë„ ê²€ìƒ‰
    try:
        top_chunks = get_top_chunks_faiss(query_embedding, top_n=5)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # 4. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ í¬í•¨)
    context = build_context(top_chunks, question=q)
    if not context:
        raise HTTPException(status_code=400, detail="ê´€ë ¨ ê°•ì˜ìžë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 5. GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
--- ê°•ì˜ìžë£Œ ë°œì·Œ ì‹œìž‘ ---
{context}
--- ê°•ì˜ìžë£Œ ë°œì·Œ ë ---

ì§ˆë¬¸: {q}
ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ìž‘ì„±í•´ì£¼ì„¸ìš”.
"""

    # 6. GPT ì‘ë‹µ ìƒì„±
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ë¶€ë“œëŸ½ê²Œ ë‹µí•´ì£¼ì„¸ìš”. "
                        "ë‹µë³€í•  ë•Œ **Markdown í¬ë§·**ì„ ì‚¬ìš©í•´ì„œ ì œëª©, ë¦¬ìŠ¤íŠ¸, ì½”ë“œë¸”ëŸ­ ë“±ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”. "
                        "ë˜í•œ ë¦¬ìŠ¤íŠ¸ í•­ëª©ì´ë‚˜ ì¤‘ìš”í•œ í¬ì¸íŠ¸ ì•žì—ëŠ” ì ì ˆí•œ ì´ëª¨ì§€(âœ…, ðŸ“Œ, ðŸš€, ðŸ‘‰ ë“±)ë¥¼ ë„£ì–´ì£¼ì„¸ìš”. "
                        "ë„ˆë¬´ ë§Žì€ ì´ëª¨ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•„ìš”í•œ ê³³ì—ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ê°€í•´ì£¼ì„¸ìš”. "
                        "ì½”ë”© ë¬¸ì œëŠ” ì •ë‹µ ëŒ€ì‹  ë‹¨ê³„ë³„ ížŒíŠ¸ë¥¼ ì£¼ê³ , ê°œë… ì„¤ëª…ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”. "
                        "ì§ˆë¬¸ì´ ê°•ì˜ìžë£Œì™€ ê´€ë ¨ì´ ì—†ìœ¼ë©´ ì •ì¤‘ížˆ ì•ˆë‚´ë§Œ í•´ì£¼ì„¸ìš”."
                    )
                },
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.7,
        )
        raw_answer = response.choices[0].message.content.strip()

        # âœ… ì¤„ë°”ê¿ˆ ë³€í™˜ ì‚­ì œ
        formatted_answer = raw_answer

        # 7. ê²°ê³¼ ì €ìž¥
        new_qa = QuestionAnswer(
            question=q,
            answer=formatted_answer,
            created_at=datetime.utcnow(),
            user_id=user_id
        )
        db.add(new_qa)
        await db.commit()

        return {"answer": formatted_answer}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPT ì‘ë‹µ ì‹¤íŒ¨: {str(e)}")
